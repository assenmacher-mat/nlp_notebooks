{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Exercise 2 - Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports\n",
    "\n",
    "In order to handle the data properly we have to import the data and the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you need to download the data set \"tweets.csv\" from the GitHub repository https://github.com/assenmacher-mat/nlp_notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook on colab ( https://colab.research.google.com/ ), you also need to run the next chunk in order to upload the data to colab.  \n",
    "Choose it in the upload window and in it will be available on colab from now on.__  \n",
    "(If you are running this notebook locally on your machine, you can skip the execution of this chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data set and perform pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook locally on your machine, you might need to adjust the path (depending on where you've saved the data).__  \n",
    "(If you are running this notebook on colab, you can can leave the path unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv(\"trump.csv\")\n",
    "tweets_raw = [tweet for tweet in list(tweet_data.text)]\n",
    "tweets = [doc.lower() for doc in tweets_raw]\n",
    "tweets = [re.sub(r\"https://.*|“|”|@\", \"\", doc) for doc in tweets]\n",
    "tweets = [re.sub(r\"[\\)\\(\\.\\,;:!?\\+\\-\\_\\#\\'\\*\\§\\$\\%\\&]\", \"\", doc) for doc in tweets]\n",
    "tweets = [nltk.word_tokenize(doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After all, we can finally start with the modeling part!  \n",
    "(If you want to have a look at the help page, just execute the following chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we determine the number of CPUs that are available on our machine  \n",
    "(The more cores are available, the faster we can train our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set by transforming every tweet to a TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tweets = [TaggedDocument(words = d, tags = [\"doc_\" + str(i)]) for i, d in enumerate(tweets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a \"tagged\" tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['at', 'the', 'request', 'of', 'senthomtillis', 'i', 'have', 'declared', 'a', 'major', 'disaster', 'for', 'the', 'great', 'state', 'of', 'north', 'carolina', 'to', 'help', 'with', 'damages', 'from', 'hurricane', 'dorian', 'assistance', 'now', 'unlocked', 'to', 'recover', 'stronger', 'than', 'ever', 'thom', 'loves', 'nc', 'and', 'so', 'do', 'i'], tags=['doc_0'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Task:  \n",
    "Think about assigning multiple tags to each of the tweets. \n",
    "This could be interesting, if we had tweets from different politicians and wanted to learn additional representations for their style of tweeting.  \n",
    "Try to assign a document identifier as well as the label $donald\\_trump$ to all our tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['at', 'the', 'request', 'of', 'senthomtillis', 'i', 'have', 'declared', 'a', 'major', 'disaster', 'for', 'the', 'great', 'state', 'of', 'north', 'carolina', 'to', 'help', 'with', 'damages', 'from', 'hurricane', 'dorian', 'assistance', 'now', 'unlocked', 'to', 'recover', 'stronger', 'than', 'ever', 'thom', 'loves', 'nc', 'and', 'so', 'do', 'i'], tags=['doc_0', 'donald_trump'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_tagged_tweets = [TaggedDocument(words = d, tags = [\"doc_\" + str(i), \"donald_trump\"]) for i, d in enumerate(tweets)]\n",
    "two_tagged_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model parameters for the Distributed memory model  \n",
    "(Now again with the corpus which documents are only assigned one tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(dm = 1, dm_concat = 0, vector_size = 100, alpha = 0.025, min_alpha = 0.0001, \n",
    "                    window = 5, min_count = 5, sample = 0.001, negative = 5, workers = cpus - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model with our twitter data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.build_vocab(documents = tagged_tweets, update = False, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model:  \n",
    "(Hint: If you want to compre the runtime of the model for different number of cores or epochs, just put \"%timeit\" in front of the command  \n",
    " in the next chunk. You will then get an evaluation of how long the process takes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.19 s\n"
     ]
    }
   ],
   "source": [
    "%time d2v_model.train(documents = tagged_tweets, total_examples = d2v_model.corpus_count, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose a document and display it as a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the washington times ukraine envoy blows ‘ massive hole ’ into democrat accusations republicans at hearing find no trump pressure the ukrainian president also strongly stated that no pressure was put on him case closed'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tagged_tweets[10].words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the most similar tweets to the one you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc_2688', 0.9226269721984863),\n",
       " ('doc_132', 0.9198408126831055),\n",
       " ('doc_3014', 0.9137082099914551)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_similar = d2v_model.docvecs.most_similar([\"doc_10\"], topn = 3)\n",
    "ids_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the whistleblower ’ s complaint is completely different and at odds from my actual conversation with the new president of ukraine the socalled whistleblower knew practically nothing in that those ridiculous charges were far more dramatic amp wrong just like liddle ’ adam schiff',\n",
       " 'john solomon as russia collusion fades ukrainian plot to help clinton emerges seanhannity foxnews',\n",
       " 'disgraced fbi acting director andrew mccabe pretends to be a poor little angel when in fact he was a big part of the crooked hillary scandal amp the russia hoax a puppet for leakin ’ james comey ig report on mccabe was devastating part of insurance policy in case i won']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(tweet.words) for tweet in tagged_tweets if tweet.tags[0] in [id[0] for id in ids_similar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9226269"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.similarity(\"doc_10\", \"doc_2688\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Distributed Bag-of-words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.43 s\n"
     ]
    }
   ],
   "source": [
    "dbow_model = Doc2Vec(dm = 0, dm_concat = 0, vector_size = 100, alpha = 0.025, min_alpha = 0.0001, \n",
    "                    window = 5, min_count = 5, sample = 0.001, negative = 5, workers = cpus - 1)\n",
    "dbow_model.build_vocab(documents = tagged_tweets, update = False, progress_per = 10000)\n",
    "%time dbow_model.train(documents = tagged_tweets, total_examples = dbow_model.corpus_count, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare how well the two models were able to learn meaningful word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dems', 0.936676561832428),\n",
       " ('radical', 0.8013758063316345),\n",
       " ('left', 0.7550336122512817),\n",
       " ('trying', 0.7318845987319946),\n",
       " ('republicans', 0.7197769284248352),\n",
       " ('facts', 0.7146976590156555),\n",
       " ('they', 0.7040835618972778),\n",
       " ('end', 0.648250937461853),\n",
       " ('congress', 0.6472545862197876),\n",
       " ('asking', 0.647139310836792)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.wv.most_similar(positive = [\"democrats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2016', 0.30525755882263184),\n",
       " ('fishing', 0.2913818955421448),\n",
       " ('product', 0.28849905729293823),\n",
       " ('times', 0.2806693911552429),\n",
       " ('judiciary', 0.2791927456855774),\n",
       " ('proved', 0.2754194438457489),\n",
       " ('debt', 0.2723675072193146),\n",
       " ('announcement', 0.26838499307632446),\n",
       " ('question', 0.2646797299385071),\n",
       " ('or', 0.26391512155532837)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbow_model.wv.most_similar(positive = [\"democrats\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now train a second Distributed Bag-of-words model and set the ``dbow_words``-option to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.29 s\n"
     ]
    }
   ],
   "source": [
    "dbow2_model = Doc2Vec(dm = 0, dm_concat = 0, vector_size = 100, alpha = 0.025, min_alpha = 0.0001, dbow_words = 1,\n",
    "                    window = 5, min_count = 5, sample = 0.001, negative = 5, workers = cpus - 1)\n",
    "dbow2_model.build_vocab(documents = tagged_tweets, update = False, progress_per = 10000)\n",
    "%time dbow2_model.train(documents = tagged_tweets, total_examples = dbow2_model.corpus_count, epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dems', 0.6945326328277588),\n",
       " ('democrat', 0.5769639015197754),\n",
       " ('committees', 0.5477724075317383),\n",
       " ('partner', 0.4884350895881653),\n",
       " ('loopholes', 0.48344436287879944),\n",
       " ('congresswomen', 0.47495728731155396),\n",
       " ('investigations', 0.47154688835144043),\n",
       " ('13', 0.47105997800827026),\n",
       " ('cities', 0.46607354283332825),\n",
       " ('radical', 0.46355095505714417)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbow2_model.wv.most_similar(positive = [\"democrats\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
