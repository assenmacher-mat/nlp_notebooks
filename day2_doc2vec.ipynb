{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Exercise 2 - Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports\n",
    "\n",
    "In order to handle the data properly we have to import the data and the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you need to download the data set \"tweets.csv\" from the GitHub repository https://github.com/assenmacher-mat/nlp_notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook on colab ( https://colab.research.google.com/ ), you also need to run the next chunk in order to upload the data to colab.  \n",
    "Choose it in the upload window and in it will be available on colab from now on.__  \n",
    "(If you are running this notebook locally on your machine, you can skip the execution of this chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook locally on your machine, you might need to adjust the path (depending on where you've saved the data).__  \n",
    "(If you are running this notebook on colab, you can can leave the path unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv(\"trump.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, just have a look at the data set in order to see what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>At the request of @SenThomTillis I have declar...</td>\n",
       "      <td>10-04-2019 21:59:44</td>\n",
       "      <td>8562</td>\n",
       "      <td>36356</td>\n",
       "      <td>False</td>\n",
       "      <td>1180241114403610626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Under my Administration Medicare Advantage pre...</td>\n",
       "      <td>10-04-2019 21:57:17</td>\n",
       "      <td>15248</td>\n",
       "      <td>54729</td>\n",
       "      <td>False</td>\n",
       "      <td>1180240498478534658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>WOW this is big stuff! https://t.co/H12yxMfua3</td>\n",
       "      <td>10-04-2019 19:46:59</td>\n",
       "      <td>15655</td>\n",
       "      <td>50526</td>\n",
       "      <td>False</td>\n",
       "      <td>1180207709985165313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>“I think it’s outrages that a Whistleblower is...</td>\n",
       "      <td>10-04-2019 14:12:23</td>\n",
       "      <td>19441</td>\n",
       "      <td>73966</td>\n",
       "      <td>False</td>\n",
       "      <td>1180123504924151809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text  \\\n",
       "0  Twitter for iPhone  At the request of @SenThomTillis I have declar...   \n",
       "1  Twitter for iPhone  Under my Administration Medicare Advantage pre...   \n",
       "2  Twitter for iPhone     WOW this is big stuff! https://t.co/H12yxMfua3   \n",
       "3  Twitter for iPhone  “I think it’s outrages that a Whistleblower is...   \n",
       "\n",
       "            created_at  retweet_count  favorite_count  is_retweet  \\\n",
       "0  10-04-2019 21:59:44           8562           36356       False   \n",
       "1  10-04-2019 21:57:17          15248           54729       False   \n",
       "2  10-04-2019 19:46:59          15655           50526       False   \n",
       "3  10-04-2019 14:12:23          19441           73966       False   \n",
       "\n",
       "                id_str  \n",
       "0  1180241114403610626  \n",
       "1  1180240498478534658  \n",
       "2  1180207709985165313  \n",
       "3  1180123504924151809  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.loc[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the tweets to a list of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw = [tweet for tweet in list(tweet_data.text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display one exemplary tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under my Administration Medicare Advantage premiums next year will be their lowest in the last 13 years. We are providing GREAT healthcare to our Seniors. We cannot let the radical socialists take that away through Medicare for All!\n"
     ]
    }
   ],
   "source": [
    "print(tweets_raw[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the basic preprocessing steps before we continue:\n",
    "    - everything to lowercase\n",
    "    - expand contractions\n",
    "    - delete url adresses\n",
    "    - delete other unwanted tokens\n",
    "    - tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [doc.lower() for doc in tweets_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = [(r\"don't\", \"do not\"), (r\"isn't\", \"is not\")]\n",
    "def expand(text, contractions = conts):\n",
    "    for c in contractions:\n",
    "        t = re.sub(c[0], c[1], text)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader mccarthy we look forward to you soon becoming speaker of the house. the do nothing dems don’t have a chance! https://t.co/uwpdgjg99f\n",
      "leader mccarthy we look forward to you soon becoming speaker of the house. the do nothing test don’t have a chance! https://t.co/uwpdgjg99f\n"
     ]
    }
   ],
   "source": [
    "print(tweets[33])\n",
    "print(expand(tweets[33]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a list of unique tokens that are at the moment present in our corpus\n",
    "### Based on this, we can identify which tokens occur the we potentially want to exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'case\", \"'collusion\", \"'could\", \"'crisis\", \"'forgotten\", \"'god\", \"'right\", \"'s\", \"'spying\", \"'ve\", '(', ')', '+', '-', '--', '.', '..', '...', '..again', '..all', '..also', '..amounts', '..are', '..between', '..breaking', '..but', '..call', '..came', '..chairman', '..comcast', '..congresswomen', '..deferral', '..despite', '..if', '..mexico', '..much', '..my', '..news', '..nice', '..not', '..now', '..on', '..other', '..saying', '..shouting', '..sorry', '..spread', '..thank', '..that', '..the', '..there', '..this', '..to', '..tv', '..united', '..was', '..we', '..who', '..why', '..willing', '..years', '.33000', '.a', '.about', '.adds', '.after', '.again', '.agricultural', '.alabama', '.alex', '.all', '.almost', '.also', '.alternative', '.amendment.', '.amounts', '.an', '.and', '.another', '.are', '.as', '.asking', '.at', '.average', '.back', '.bad', '.based', '.be', '.became', '.because', '.best', '.better', '.between']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(set(nltk.word_tokenize(\" \".join(tweets))))[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [re.sub(r\"https://.*|“|”|@\", \"\", doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [re.sub(r\"[\\)\\(\\.\\,;:!?\\+\\-\\_\\#\\'\\*\\§\\$\\%\\&]\", \"\", doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [nltk.word_tokenize(doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"''\", '0', '03', '09', '1', '1/1024th', '1/2', '10', '100', '1000', '1000/24th', '10000', '100000', '1000000', '1036', '104th', '105', '107', '10th', '11', '11000000', '1112', '1130', '11th', '12', '122', '125th', '12th', '13', '133000', '135', '138', '14', '145', '14th', '15', '150', '1500', '150th', '157005000', '158000000', '15th', '16', '160th', '17', '170', '17000', '170000', '18', '180', '1800', '1874', '18959495168', '18th', '19', '191', '1951', '196000', '1969', '1970s', '1972', '1976', '1977', '1980', '1984', '1990', '1994', '1997', '1998', '19th', '1st', '2', '20', '200', '2000', '20000', '2001', '2002', '2005', '2010', '2011', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2020takebackthehouse', '2021', '2024', '205', '20th', '21', '21st', '22', '223306', '23', '232', '24']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(set([word for tweet in tweets for word in tweet]))[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After all, we can finally start with the modeling part!  \n",
    "(If you want to have a look at the help page, just execute the following chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Doc2Vec in module gensim.models.doc2vec:\n",
      "\n",
      "class Doc2Vec(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Class for training, using and evaluating neural networks described in\n",
      " |  `Distributed Representations of Sentences and Documents <http://arxiv.org/abs/1405.4053v2>`_.\n",
      " |  \n",
      " |  Some important internal attributes are the following:\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      " |      directly to query those embeddings in various ways. See the module level docstring for examples.\n",
      " |  \n",
      " |  docvecs : :class:`~gensim.models.keyedvectors.Doc2VecKeyedVectors`\n",
      " |      This object contains the paragraph vectors. Remember that the only difference between this model and\n",
      " |      :class:`~gensim.models.word2vec.Word2Vec` is that besides the word vectors we also include paragraph embeddings\n",
      " |      to capture the paragraph.\n",
      " |  \n",
      " |      In this way we can capture the difference between the same word used in a different context.\n",
      " |      For example we now have a different representation of the word \"leaves\" in the following two sentences ::\n",
      " |  \n",
      " |          1. Manos leaves the office every day at 18:00 to catch his train\n",
      " |          2. This season is called Fall, because leaves fall from the trees.\n",
      " |  \n",
      " |      In a plain :class:`~gensim.models.word2vec.Word2Vec` model the word would have exactly the same representation\n",
      " |      in both sentences, in :class:`~gensim.models.doc2vec.Doc2Vec` it will not.\n",
      " |  \n",
      " |  vocabulary : :class:`~gensim.models.doc2vec.Doc2VecVocab`\n",
      " |      This object represents the vocabulary (sometimes called Dictionary in gensim) of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      sorting words by frequency, or discarding extremely rare words.\n",
      " |  \n",
      " |  trainables : :class:`~gensim.models.doc2vec.Doc2VecTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. The semantics of the\n",
      " |      network differ slightly in the two available training modes (CBOW or SG) but you can think of it as a NN with\n",
      " |      a single projection and hidden layer which we train on the corpus. The weights are then used as our embeddings\n",
      " |      The only addition to the underlying NN used in :class:`~gensim.models.word2vec.Word2Vec` is that the input\n",
      " |      includes not only the word vectors of each word in the context, but also the paragraph vector.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Doc2Vec\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getitem__(self, tag)\n",
      " |      Get the vector representation of (possible multi-term) tag.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : {str, int, list of str, list of int}\n",
      " |          The tag (or tags) to be looked up in the model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      np.ndarray\n",
      " |          The vector representations of each tag as a matrix (will be 1D if `tag` was a single tag)\n",
      " |  \n",
      " |  __init__(self, documents=None, corpus_file=None, dm_mean=None, dm=1, dbow_words=0, dm_concat=0, dm_tag_count=1, docvecs=None, docvecs_mapfile=None, comment=None, trim_rule=None, callbacks=(), **kwargs)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      " |          Input corpus, can be simply a list of elements, but for larger corpora,consider an iterable that streams\n",
      " |          the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\n",
      " |          left uninitialized -- use if you plan to initialize it in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |          Documents' tags are assigned automatically and are equal to line number, as in\n",
      " |          :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      " |      dm : {1,0}, optional\n",
      " |          Defines the training algorithm. If `dm=1`, 'distributed memory' (PV-DM) is used.\n",
      " |          Otherwise, `distributed bag of words` (PV-DBOW) is employed.\n",
      " |      vector_size : int, optional\n",
      " |          Dimensionality of the feature vectors.\n",
      " |      window : int, optional\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling.\n",
      " |          In Python 3, reproducibility between interpreter launches also requires use of the `PYTHONHASHSEED`\n",
      " |          environment variable to control hash randomization.\n",
      " |      min_count : int, optional\n",
      " |          Ignores all words with total frequency lower than this.\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      hs : {1,0}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      dm_mean : {1,0}, optional\n",
      " |          If 0 , use the sum of the context word vectors. If 1, use the mean.\n",
      " |          Only applies when `dm` is used in non-concatenative mode.\n",
      " |      dm_concat : {1,0}, optional\n",
      " |          If 1, use concatenation of context vectors rather than sum/average;\n",
      " |          Note concatenation results in a much-larger model, as the input\n",
      " |          is no longer the size of one (sampled or arithmetically combined) word vector, but the\n",
      " |          size of the tag(s) and all words in the context strung together.\n",
      " |      dm_tag_count : int, optional\n",
      " |          Expected constant number of document tags per document, when using\n",
      " |          dm_concat mode.\n",
      " |      dbow_words : {1,0}, optional\n",
      " |          If set to 1 trains word-vectors (in skip-gram fashion) simultaneous with DBOW\n",
      " |          doc-vector training; If 0, only trains doc-vectors (faster).\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Abbreviated name reflecting major configuration parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          Human readable representation of the models internal state.\n",
      " |  \n",
      " |  build_vocab(self, documents=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of documents (can be a once-only generator stream).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      " |          Can be simply a list of :class:`~gensim.models.doc2vec.TaggedDocument` elements, but for larger corpora,\n",
      " |          consider an iterable that streams the documents directly from disk/network.\n",
      " |          See :class:`~gensim.models.doc2vec.TaggedBrownCorpus` or :class:`~gensim.models.doc2vec.TaggedLineDocument`\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\n",
      " |          and are equal to a line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      " |      update : bool\n",
      " |          If true, the new words in `documents` will be added to model's vocab.\n",
      " |      progress_per : int\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional key word arguments passed to the internal vocabulary construction.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Build model vocabulary from a passed dictionary that contains a (word -> word count) mapping.\n",
      " |      Words must be of type unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          Word <-> count mapping.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.doc2vec.Doc2Vec.build_vocab` and is not stored as part of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Resets the current word vectors.\n",
      " |  \n",
      " |  delete_temporary_training_data(self, keep_doctags_vectors=True, keep_inference=True)\n",
      " |      Discard parameters that are used in training and score. Use if you're sure you're done training a model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      keep_doctags_vectors : bool, optional\n",
      " |          Set to False if you don't want to save doctags vectors. In this case you will not be able to use\n",
      " |          :meth:`~gensim.models.keyedvectors.Doc2VecKeyedVectors.most_similar`,\n",
      " |          :meth:`~gensim.models.keyedvectors.Doc2VecKeyedVectors.similarity`, etc methods.\n",
      " |      keep_inference : bool, optional\n",
      " |          Set to False if you don't want to store parameters that are used for\n",
      " |          :meth:`~gensim.models.doc2vec.Doc2Vec.infer_vector` method.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of raw words in the vocabulary.\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the **specific** model's memory consuming members\n",
      " |          to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |          Includes members from the base classes as well as weights and tag lookup memory estimation specific to the\n",
      " |          class.\n",
      " |  \n",
      " |  estimated_lookup_memory(self)\n",
      " |      Get estimated memory for tag lookup, 0 if using pure int tags.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          The estimated RAM required to look up a tag in bytes.\n",
      " |  \n",
      " |  infer_vector(self, doc_words, alpha=None, min_alpha=None, epochs=None, steps=None)\n",
      " |      Infer a vector for given post-bulk training document.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Subsequent calls to this function may infer different representations for the same document.\n",
      " |      For a more stable representation, increase the number of steps to assert a stricket convergence.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      doc_words : list of str\n",
      " |          A document for which the vector representation will be inferred.\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate. If unspecified, value from model initialization will be reused.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` over all inference epochs. If unspecified,\n",
      " |          value from model initialization will be reused.\n",
      " |      epochs : int, optional\n",
      " |          Number of times to train the new document. Larger values take more time, but may improve\n",
      " |          quality and run-to-run stability of inferred vectors. If unspecified, the `epochs` value\n",
      " |          from model initialization will be reused.\n",
      " |      steps : int, optional, deprecated\n",
      " |          Previous name for `epochs`, still available for now for backward compatibility: if\n",
      " |          `epochs` is unspecified but `steps` is, the `steps` value will be used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      np.ndarray\n",
      " |          The inferred paragraph vector for the new document.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Pre-compute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True - forget the original vectors and only keep the normalized ones to saved RAM (also you can't\n",
      " |          continue training if call it with `replace=True`).\n",
      " |  \n",
      " |  reset_from(self, other_model)\n",
      " |      Copy shareable data structures from another (possibly pre-trained) model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other_model : :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      " |          Other model whose internal data structures will be copied over to the current object.\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, doctag_vec=False, word_vec=True, prefix='*dt_', fvocab=None, binary=False)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original C word2vec-tool.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in.\n",
      " |      doctag_vec : bool, optional\n",
      " |          Indicates whether to store document vectors.\n",
      " |      word_vec : bool, optional\n",
      " |          Indicates whether to store word vectors.\n",
      " |      prefix : str, optional\n",
      " |          Uniquely identifies doctags from word vocab, and avoids collision in case of repeated string in doctag\n",
      " |          and word vocab.\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary.\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, otherwise - will be saved in plain text.\n",
      " |  \n",
      " |  train(self, documents=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=())\n",
      " |      Update the model's neural weights.\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of documents) or `total_words` (count of\n",
      " |      raw words in documents) **MUST** be provided. If `documents` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.word2vec.Word2Vec.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.word2vec.Word2Vec.train` is only called once,\n",
      " |      you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      documents : iterable of list of :class:`~gensim.models.doc2vec.TaggedDocument`, optional\n",
      " |          Can be simply a list of elements, but for larger corpora,consider an iterable that streams\n",
      " |          the documents directly from disk/network. If you don't supply `documents` (or `corpus_file`), the model is\n",
      " |          left uninitialized -- use if you plan to initialize it in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `documents` to get performance boost. Only one of `documents` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them). Documents' tags are assigned automatically\n",
      " |          and are equal to line number, as in :class:`~gensim.models.doc2vec.TaggedLineDocument`.\n",
      " |      total_examples : int, optional\n",
      " |          Count of documents.\n",
      " |      total_words : int, optional\n",
      " |          Count of raw words in documents.\n",
      " |      epochs : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to `train`.\n",
      " |          Use only if making multiple calls to `train`, when you want to manage the alpha learning-rate yourself\n",
      " |          (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to\n",
      " |          :meth:`~gensim.models.doc2vec.Doc2Vec.train`.\n",
      " |          Use only if making multiple calls to :meth:`~gensim.models.doc2vec.Doc2Vec.train`, when you want to manage\n",
      " |          the alpha learning-rate yourself (not recommended).\n",
      " |      word_count : int, optional\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in documents.\n",
      " |      queue_factor : int, optional\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float, optional\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved :class:`~gensim.models.doc2vec.Doc2Vec` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      *args : object\n",
      " |          Additional arguments, see `~gensim.models.base_any2vec.BaseWordEmbeddingsModel.load`.\n",
      " |      **kwargs : object\n",
      " |          Additional arguments, see `~gensim.models.base_any2vec.BaseWordEmbeddingsModel.load`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.doc2vec.Doc2Vec.save`\n",
      " |          Save :class:`~gensim.models.doc2vec.Doc2Vec` model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.doc2vec.Doc2Vec`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  dbow\n",
      " |      Indicates whether 'distributed bag of words' (PV-DBOW) will be used, else 'distributed memory'\n",
      " |      (PV-DM) is used.\n",
      " |  \n",
      " |  dm\n",
      " |      Indicates whether 'distributed memory' (PV-DM) will be used, else 'distributed bag of words'\n",
      " |      (PV-DBOW) is used.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseAny2VecModel:\n",
      " |  \n",
      " |  save(self, fname_or_handle, **kwargs)\n",
      " |      \"Save the object to file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname_or_handle : {str, file-like object}\n",
      " |          Path to file where the model will be persisted.\n",
      " |      **kwargs : object\n",
      " |          Key word arguments propagated to :meth:`~gensim.utils.SaveLoad.save`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.base_any2vec.BaseAny2VecModel.load`\n",
      " |          Method for load model after current method.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we determine the number of CPUs that are available on our machine  \n",
    "(The more cores are available, the faster we can train our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tweets = [TaggedDocument(words = d, tags = [\"doc_\" + str(i)]) for i, d in enumerate(tweets)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display a \"tagged\" tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['at', 'the', 'request', 'of', 'senthomtillis', 'i', 'have', 'declared', 'a', 'major', 'disaster', 'for', 'the', 'great', 'state', 'of', 'north', 'carolina', 'to', 'help', 'with', 'damages', 'from', 'hurricane', 'dorian', 'assistance', 'now', 'unlocked', 'to', 'recover', 'stronger', 'than', 'ever', 'thom', 'loves', 'nc', 'and', 'so', 'do', 'i'], tags=['doc_0'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Task:  \n",
    "Think about assigning multiple tags to each of the tweets.  \n",
    "This could be interesting, if we had tweets from different politicians and wanted to learn additional representations for their style of tweeting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['at', 'the', 'request', 'of', 'senthomtillis', 'i', 'have', 'declared', 'a', 'major', 'disaster', 'for', 'the', 'great', 'state', 'of', 'north', 'carolina', 'to', 'help', 'with', 'damages', 'from', 'hurricane', 'dorian', 'assistance', 'now', 'unlocked', 'to', 'recover', 'stronger', 'than', 'ever', 'thom', 'loves', 'nc', 'and', 'so', 'do', 'i'], tags=['doc_0', 'donald_trump'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_tagged_tweets = [TaggedDocument(words = d, tags = [\"doc_\" + str(i), \"donald_trump\"]) for i, d in enumerate(tweets)]\n",
    "two_tagged_tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec(dm = 1, dm_concat = 0, vector_size = 100, alpha = 0.025, min_alpha = 0.0001, \n",
    "                    window = 5, min_count = 5, sample = 0.001, negative = 5, workers = cpus - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model with our twitter data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.build_vocab(documents = tagged_tweets, update = False, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model:  \n",
    "(Hint: If you want to compre the runtime of the model for different number of cores or epochs, just put \"%timeit\" in front of the command  \n",
    " in the next chunk. You will then get an evaluation of how long the process takes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "%time d2v_model.train(documents = tagged_tweets, total_examples = d2v_model.corpus_count, epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chose a document and display it as a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the washington times ukraine envoy blows ‘ massive hole ’ into democrat accusations republicans at hearing find no trump pressure the ukrainian president also strongly stated that no pressure was put on him case closed'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tagged_tweets[10].words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the most similar tweets to the one you chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc_3177', 0.8913683295249939),\n",
       " ('doc_1821', 0.870488166809082),\n",
       " ('doc_2688', 0.8682408332824707)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_similar = d2v_model.docvecs.most_similar([\"doc_10\"], topn = 3)\n",
    "ids_similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john solomon factual errors and major omissions in the mueller report show that it is totally biased against trump',\n",
       " 'john solomon as russia collusion fades ukrainian plot to help clinton emerges seanhannity foxnews',\n",
       " 'former fbi top lawyer james baker just admitted involvement in fisa warrant and further admitted there were irregularities in the way the russia probe was handled they relied heavily on the unverified trump dossier paid for by the dnc amp clinton campaign amp funded through a']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(tweet.words) for tweet in tagged_tweets if tweet.tags[0] in [id[0] for id in ids_similar]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8704882"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.docvecs.similarity(\"doc_10\", \"doc_1821\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now: Explore your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('afghanistan', 0.9131811261177063), ('cases', 0.9091619253158569), ('hispanics', 0.8991455435752869), ('decisions', 0.8955312967300415), ('european', 0.8937901258468628), ('parts', 0.8920109272003174), ('ahead', 0.884681224822998), ('lowering', 0.8811957836151123), ('records', 0.8771611452102661), ('technology', 0.8753639459609985)]\n",
      "[('hillary', 0.9773930311203003), ('crooked', 0.9718740582466125), ('dnc', 0.8628735542297363), ('deleted', 0.8587695360183716), ('comey', 0.8489434719085693), ('james', 0.8456957340240479), ('bob', 0.8333559036254883), ('ig', 0.8320258855819702), ('dirty', 0.8250784277915955), ('13', 0.818506121635437)]\n",
      "[('dems', 0.9202031493186951), ('radical', 0.8052688837051392), ('left', 0.747321367263794), ('they', 0.7006311416625977), ('end', 0.6940463185310364), ('congresswomen', 0.6931781768798828), ('trying', 0.6682571172714233), ('facts', 0.6605888605117798), ('thinking', 0.6578940749168396), ('hearings', 0.6559553146362305)]\n",
      "[('large', 0.7701966762542725), ('illegals', 0.7650654315948486), ('tariffs', 0.7538250684738159), ('flow', 0.7466623783111572), ('caravans', 0.7304707765579224), ('system', 0.7214885354042053), ('tariff', 0.7212337851524353), ('steel', 0.718453049659729), ('us', 0.7175641059875488), ('close', 0.712874710559845)]\n",
      "[('tariffs', 0.8050028681755066), ('paying', 0.7555699348449707), ('helping', 0.7486938238143921), ('iran', 0.7365538477897644), ('eu', 0.7277566194534302), ('trade', 0.7238926887512207), ('charged', 0.7229114770889282), ('taking', 0.713096559047699), ('us', 0.7108510136604309), ('nuclear', 0.7050739526748657)]\n",
      "[('china', 0.7873976230621338), ('large', 0.7820004224777222), ('talks', 0.7699024081230164), ('tariffs', 0.766541600227356), ('product', 0.7485625743865967), ('taking', 0.7465744614601135), ('deals', 0.7463473677635193), ('additional', 0.7222486734390259), ('eu', 0.7171304225921631), ('goods', 0.7152644991874695)]\n"
     ]
    }
   ],
   "source": [
    "print(d2v_model.wv.most_similar(positive = [\"germany\"]))\n",
    "print(d2v_model.wv.most_similar(positive = [\"clinton\"]))\n",
    "print(d2v_model.wv.most_similar(positive = [\"democrats\"]))\n",
    "print(d2v_model.wv.most_similar(positive = [\"mexico\"]))\n",
    "print(d2v_model.wv.most_similar(positive = [\"china\"]))\n",
    "print(d2v_model.wv.most_similar(positive = [\"mexico\", \"trade\"], negative = [\"wall\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the possibilities the model by e.g. switching from skip-gram to cbow, using averaging instead of concatenation, chosing a larger embedding size, more negative examples, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using ``gensim.models.phrases`` in order to form bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases(tweets, min_count=100, threshold=10)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((b'\\xe2\\x80\\x99', b't'), 40.14282388966591),\n",
       " ((b'\\xe2\\x80\\x99', b's'), 39.051824673367356),\n",
       " ((b'witch', b'hunt'), 24.661771250556296),\n",
       " ((b'will', b'be'), 15.202834497541446),\n",
       " ((b'united', b'states'), 98.71593416819549),\n",
       " ((b'thank', b'you'), 49.87788116523749),\n",
       " ((b'our', b'country'), 27.60977332033378),\n",
       " ((b'fake', b'news'), 74.46145685997172),\n",
       " ((b'don', b'\\xe2\\x80\\x99'), 16.771136693276688)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(bigram.phrasegrams.items()), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(bigram[tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9408352 , -2.9448547 , -0.0644585 ,  2.5617635 ,  0.11626738,\n",
       "        0.9916419 , -0.982047  , -0.36746535,  2.282364  ,  4.807505  ,\n",
       "       -0.57372135, -0.1776367 ,  3.6928544 , -0.07633026, -0.3486168 ,\n",
       "       -1.1801438 , -2.0244286 ,  3.9215038 ,  5.7360845 ,  1.1472751 ,\n",
       "       -0.6600361 ,  3.4406743 ,  0.42411965,  2.2777426 ,  4.44871   ,\n",
       "       -0.51950186, -3.310043  , -0.11698956,  1.4302472 , -1.8335285 ,\n",
       "       -1.2929436 , -3.6467178 ,  0.5088786 ,  0.97771716, -2.7727382 ,\n",
       "       -1.151348  , -2.8728178 ,  0.28081998,  0.15078373,  0.28773436,\n",
       "        2.4083438 ,  1.5199484 ,  0.94393754, -3.9986844 , -0.9181879 ,\n",
       "        2.4555998 , -0.99208266,  0.16184539, -2.7211847 , -0.95676094,\n",
       "       -1.9936352 ,  0.18663087,  0.7953555 , -1.7466047 ,  1.345637  ,\n",
       "        0.13392718, -0.39346752, -1.2971301 ,  1.736669  ,  1.0345834 ,\n",
       "        3.0065439 , -0.919131  , -0.42990413,  1.9712603 , -1.983691  ,\n",
       "       -0.59394854, -1.0466216 ,  2.0349941 ,  2.022733  , -0.22590888,\n",
       "        0.11601094,  4.5244036 , -1.2167819 , -1.489552  , -0.08983883,\n",
       "        0.09235708,  1.7298062 , -1.4144298 ,  4.906148  ,  1.6651174 ,\n",
       "        2.6185067 ,  1.9192597 , -1.6644619 , -0.79504645,  1.172478  ,\n",
       "        1.7839055 , -0.94101334,  5.75815   ,  3.9923134 , -2.2030773 ,\n",
       "        1.2307607 ,  0.41266924,  0.5325593 ,  0.6659662 ,  1.2994248 ,\n",
       "        0.14457057,  1.6736019 , -2.932189  , -1.2620611 , -0.7192462 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"clinton\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
