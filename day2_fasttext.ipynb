{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Exercise 3 - FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports\n",
    "\n",
    "In order to handle the data properly we have to import the data and the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you need to download the data set \"tweets.csv\" from the GitHub repository https://github.com/assenmacher-mat/nlp_notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook on colab ( https://colab.research.google.com/ ), you also need to run the next chunk in order to upload the data to colab.  \n",
    "Choose it in the upload window and in it will be available on colab from now on.__  \n",
    "(If you are running this notebook locally on your machine, you can skip the execution of this chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook locally on your machine, you might need to adjust the path (depending on where you've saved the data).__  \n",
    "(If you are running this notebook on colab, you can can leave the path unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv(\"trump.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, just have a look at the data set in order to see what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>At the request of @SenThomTillis I have declar...</td>\n",
       "      <td>10-04-2019 21:59:44</td>\n",
       "      <td>8562</td>\n",
       "      <td>36356</td>\n",
       "      <td>False</td>\n",
       "      <td>1180241114403610626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Under my Administration Medicare Advantage pre...</td>\n",
       "      <td>10-04-2019 21:57:17</td>\n",
       "      <td>15248</td>\n",
       "      <td>54729</td>\n",
       "      <td>False</td>\n",
       "      <td>1180240498478534658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>WOW this is big stuff! https://t.co/H12yxMfua3</td>\n",
       "      <td>10-04-2019 19:46:59</td>\n",
       "      <td>15655</td>\n",
       "      <td>50526</td>\n",
       "      <td>False</td>\n",
       "      <td>1180207709985165313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>“I think it’s outrages that a Whistleblower is...</td>\n",
       "      <td>10-04-2019 14:12:23</td>\n",
       "      <td>19441</td>\n",
       "      <td>73966</td>\n",
       "      <td>False</td>\n",
       "      <td>1180123504924151809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source                                               text  \\\n",
       "0  Twitter for iPhone  At the request of @SenThomTillis I have declar...   \n",
       "1  Twitter for iPhone  Under my Administration Medicare Advantage pre...   \n",
       "2  Twitter for iPhone     WOW this is big stuff! https://t.co/H12yxMfua3   \n",
       "3  Twitter for iPhone  “I think it’s outrages that a Whistleblower is...   \n",
       "\n",
       "            created_at  retweet_count  favorite_count  is_retweet  \\\n",
       "0  10-04-2019 21:59:44           8562           36356       False   \n",
       "1  10-04-2019 21:57:17          15248           54729       False   \n",
       "2  10-04-2019 19:46:59          15655           50526       False   \n",
       "3  10-04-2019 14:12:23          19441           73966       False   \n",
       "\n",
       "                id_str  \n",
       "0  1180241114403610626  \n",
       "1  1180240498478534658  \n",
       "2  1180207709985165313  \n",
       "3  1180123504924151809  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.loc[:3,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the tweets to a list of texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_raw = [tweet for tweet in list(tweet_data.text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display one exemplary tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under my Administration Medicare Advantage premiums next year will be their lowest in the last 13 years. We are providing GREAT healthcare to our Seniors. We cannot let the radical socialists take that away through Medicare for All!\n"
     ]
    }
   ],
   "source": [
    "print(tweets_raw[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the basic preprocessing steps before we continue:\n",
    "    - everything to lowercase\n",
    "    - expand contractions\n",
    "    - delete url adresses\n",
    "    - delete other unwanted tokens\n",
    "    - tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [doc.lower() for doc in tweets_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = [(r\"don't\", \"do not\"), (r\"isn't\", \"is not\")]\n",
    "def expand(text, contractions = conts):\n",
    "    for c in contractions:\n",
    "        t = re.sub(c[0], c[1], text)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leader mccarthy we look forward to you soon becoming speaker of the house. the do nothing dems don’t have a chance! https://t.co/uwpdgjg99f\n",
      "leader mccarthy we look forward to you soon becoming speaker of the house. the do nothing dems don’t have a chance! https://t.co/uwpdgjg99f\n"
     ]
    }
   ],
   "source": [
    "print(tweets[33])\n",
    "print(expand(tweets[33]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a list of unique tokens that are at the moment present in our corpus\n",
    "### Based on this, we can identify which tokens occur the we potentially want to exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'case\", \"'collusion\", \"'could\", \"'crisis\", \"'forgotten\", \"'god\", \"'right\", \"'s\", \"'spying\", \"'ve\", '(', ')', '+', '-', '--', '.', '..', '...', '..again', '..all', '..also', '..amounts', '..are', '..between', '..breaking', '..but', '..call', '..came', '..chairman', '..comcast', '..congresswomen', '..deferral', '..despite', '..if', '..mexico', '..much', '..my', '..news', '..nice', '..not', '..now', '..on', '..other', '..saying', '..shouting', '..sorry', '..spread', '..thank', '..that', '..the', '..there', '..this', '..to', '..tv', '..united', '..was', '..we', '..who', '..why', '..willing', '..years', '.33000', '.a', '.about', '.adds', '.after', '.again', '.agricultural', '.alabama', '.alex', '.all', '.almost', '.also', '.alternative', '.amendment.', '.amounts', '.an', '.and', '.another', '.are', '.as', '.asking', '.at', '.average', '.back', '.bad', '.based', '.be', '.became', '.because', '.best', '.better', '.between']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(set(nltk.word_tokenize(\" \".join(tweets))))[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [re.sub(r\"https://.*|“|”|@\", \"\", doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [re.sub(r\"[\\)\\(\\.\\,;:!?\\+\\-\\_\\#\\'\\*\\§\\$\\%\\&]\", \"\", doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [nltk.word_tokenize(doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"''\", '0', '03', '09', '1', '1/1024th', '1/2', '10', '100', '1000', '1000/24th', '10000', '100000', '1000000', '1036', '104th', '105', '107', '10th', '11', '11000000', '1112', '1130', '11th', '12', '122', '125th', '12th', '13', '133000', '135', '138', '14', '145', '14th', '15', '150', '1500', '150th', '157005000', '158000000', '15th', '16', '160th', '17', '170', '17000', '170000', '18', '180', '1800', '1874', '18959495168', '18th', '19', '191', '1951', '196000', '1969', '1970s', '1972', '1976', '1977', '1980', '1984', '1990', '1994', '1997', '1998', '19th', '1st', '2', '20', '200', '2000', '20000', '2001', '2002', '2005', '2010', '2011', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2020takebackthehouse', '2021', '2024', '205', '20th', '21', '21st', '22', '223306', '23', '232', '24']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(set([word for tweet in tweets for word in tweet]))[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After all, we can finally start with the modeling part!  \n",
    "(If you want to have a look at the help page, just execute the following chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FastText in module gensim.models.fasttext:\n",
      "\n",
      "class FastText(gensim.models.base_any2vec.BaseWordEmbeddingsModel)\n",
      " |  Train, use and evaluate word representations learned using the method\n",
      " |  described in `Enriching Word Vectors with Subword Information <https://arxiv.org/abs/1607.04606>`_, aka FastText.\n",
      " |  \n",
      " |  The model can be stored/loaded via its :meth:`~gensim.models.fasttext.FastText.save` and\n",
      " |  :meth:`~gensim.models.fasttext.FastText.load` methods, or loaded from a format compatible with the original\n",
      " |  Fasttext implementation via :meth:`~gensim.models.fasttext.FastText.load_fasttext_format`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  wv : :class:`~gensim.models.keyedvectors.FastTextKeyedVectors`\n",
      " |      This object essentially contains the mapping between words and embeddings. These are similar to the embeddings\n",
      " |      computed in the :class:`~gensim.models.word2vec.Word2Vec`, however here we also include vectors for n-grams.\n",
      " |      This allows the model to compute embeddings even for **unseen** words (that do not exist in the vocabulary),\n",
      " |      as the aggregate of the n-grams included in the word. After training the model, this attribute can be used\n",
      " |      directly to query those embeddings in various ways. Check the module level docstring for some examples.\n",
      " |  vocabulary : :class:`~gensim.models.fasttext.FastTextVocab`\n",
      " |      This object represents the vocabulary of the model.\n",
      " |      Besides keeping track of all unique words, this object provides extra functionality, such as\n",
      " |      constructing a huffman tree (frequent words are closer to the root), or discarding extremely rare words.\n",
      " |  trainables : :class:`~gensim.models.fasttext.FastTextTrainables`\n",
      " |      This object represents the inner shallow neural network used to train the embeddings. This is very\n",
      " |      similar to the network of the :class:`~gensim.models.word2vec.Word2Vec` model, but it also trains weights\n",
      " |      for the N-Grams (sequences of more than 1 words). The semantics of the network are almost the same as\n",
      " |      the one used for the :class:`~gensim.models.word2vec.Word2Vec` model.\n",
      " |      You can think of it as a NN with a single projection and hidden layer which we train on the corpus.\n",
      " |      The weights are then used as our embeddings. An important difference however between the two models, is the\n",
      " |      scoring function used to compute the loss. In the case of FastText, this is modified in word to also account\n",
      " |      for the internal structure of words, besides their concurrence counts.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      FastText\n",
      " |      gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
      " |      gensim.models.base_any2vec.BaseAny2VecModel\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |      Deprecated. Use self.wv.__contains__() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`gensim.models.keyedvectors.KeyedVectors.__contains__`\n",
      " |  \n",
      " |  __getitem__(self, words)\n",
      " |      Deprecated. Use self.wv.__getitem__() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`gensim.models.keyedvectors.KeyedVectors.__getitem__`\n",
      " |  \n",
      " |  __init__(self, sentences=None, corpus_file=None, sg=0, hs=0, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None, word_ngrams=1, sample=0.001, seed=1, workers=3, min_alpha=0.0001, negative=5, ns_exponent=0.75, cbow_mean=1, hashfxn=<built-in function hash>, iter=5, null_word=0, min_n=3, max_n=6, sorted_vocab=1, bucket=2000000, trim_rule=None, batch_words=10000, callbacks=(), compatible_hash=True)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |          If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      " |          in some other way.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      " |      min_count : int, optional\n",
      " |          The model ignores all words with total frequency lower than this.\n",
      " |      size : int, optional\n",
      " |          Dimensionality of the word vectors.\n",
      " |      window : int, optional\n",
      " |          The maximum distance between the current and predicted word within a sentence.\n",
      " |      workers : int, optional\n",
      " |          Use these many worker threads to train the model (=faster training with multicore machines).\n",
      " |      alpha : float, optional\n",
      " |          The initial learning rate.\n",
      " |      min_alpha : float, optional\n",
      " |          Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      " |      sg : {1, 0}, optional\n",
      " |          Training algorithm: skip-gram if `sg=1`, otherwise CBOW.\n",
      " |      hs : {1,0}, optional\n",
      " |          If 1, hierarchical softmax will be used for model training.\n",
      " |          If set to 0, and `negative` is non-zero, negative sampling will be used.\n",
      " |      seed : int, optional\n",
      " |          Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      " |          the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      " |          you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      " |          from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      " |          use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      " |      max_vocab_size : int, optional\n",
      " |          Limits the RAM during vocabulary building; if there are more unique\n",
      " |          words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      " |          Set to `None` for no limit.\n",
      " |      sample : float, optional\n",
      " |          The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      " |          useful range is (0, 1e-5).\n",
      " |      negative : int, optional\n",
      " |          If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      " |          should be drawn (usually between 5-20).\n",
      " |          If set to 0, no negative sampling is used.\n",
      " |      ns_exponent : float, optional\n",
      " |          The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      " |          to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      " |          than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      " |          More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupré, Lesaint, & Royo-Letelier suggest that\n",
      " |          other values may perform better for recommendation applications.\n",
      " |      cbow_mean : {1,0}, optional\n",
      " |          If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      " |      hashfxn : function, optional\n",
      " |          Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      " |      iter : int, optional\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of themodel.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      sorted_vocab : {1,0}, optional\n",
      " |          If 1, sort the vocabulary by descending frequency before assigning word indices.\n",
      " |      batch_words : int, optional\n",
      " |          Target size (in words) for batches of examples passed to worker threads (and\n",
      " |          thus cython routines).(Larger batches will be passed if individual\n",
      " |          texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      " |      min_n : int, optional\n",
      " |          Minimum length of char n-grams to be used for training word representations.\n",
      " |      max_n : int, optional\n",
      " |          Max length of char ngrams to be used for training word representations. Set `max_n` to be\n",
      " |          lesser than `min_n` to avoid char ngrams being used.\n",
      " |      word_ngrams : {1,0}, optional\n",
      " |          If 1, uses enriches word vectors with subword(n-grams) information.\n",
      " |          If 0, this is equivalent to :class:`~gensim.models.word2vec.Word2Vec`.\n",
      " |      bucket : int, optional\n",
      " |          Character ngrams are hashed into a fixed number of buckets, in order to limit the\n",
      " |          memory usage of the model. This option specifies the number of buckets used by the model.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      \n",
      " |      compatible_hash: bool, optional\n",
      " |          By default, newer versions of Gensim's FastText use a hash function\n",
      " |          that is 100% compatible with Facebook's FastText.\n",
      " |          Older versions were not 100% compatible due to a bug.\n",
      " |          To use the older, incompatible hash function, set this to False.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Initialize and train a `FastText` model:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = FastText(sentences, min_count=1)\n",
      " |          >>> say_vector = model.wv['say']  # get vector for word\n",
      " |          >>> of_vector = model.wv['of']  # get vector for out-of-vocab word\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=None, case_insensitive=True)\n",
      " |  \n",
      " |  build_vocab(self, sentences=None, corpus_file=None, update=False, progress_per=10000, keep_raw_vocab=False, trim_rule=None, **kwargs)\n",
      " |      Build vocabulary from a sequence of sentences (can be a once-only generator stream).\n",
      " |      Each sentence must be a list of unicode strings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          Can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      " |          `corpus_file` arguments need to be passed (not both of them).\n",
      " |      update : bool\n",
      " |          If true, the new words in `sentences` will be added to model's vocab.\n",
      " |      progress_per : int\n",
      " |          Indicates how many words to process before showing/updating the progress.\n",
      " |      keep_raw_vocab : bool\n",
      " |          If not true, delete the raw vocabulary after the scaling is done and free up RAM.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during\n",
      " |          :meth:`~gensim.models.fasttext.FastText.build_vocab` and is not stored as part of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional key word parameters passed to\n",
      " |          :meth:`~gensim.models.base_any2vec.BaseWordEmbeddingsModel.build_vocab`.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Train a model and update vocab for online training:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> sentences_1 = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>> sentences_2 = [[\"dude\", \"say\", \"wazzup!\"]]\n",
      " |          >>>\n",
      " |          >>> model = FastText(min_count=1)\n",
      " |          >>> model.build_vocab(sentences_1)\n",
      " |          >>> model.train(sentences_1, total_examples=model.corpus_count, epochs=model.epochs)\n",
      " |          >>>\n",
      " |          >>> model.build_vocab(sentences_2, update=True)\n",
      " |          >>> model.train(sentences_2, total_examples=model.corpus_count, epochs=model.epochs)\n",
      " |  \n",
      " |  clear_sims(self)\n",
      " |      Remove all L2-normalized word vectors from the model, to free up memory.\n",
      " |      \n",
      " |      You can recompute them later again using the :meth:`~gensim.models.fasttext.FastText.init_sims` method.\n",
      " |  \n",
      " |  estimate_memory(self, vocab_size=None, report=None)\n",
      " |      Estimate required memory for a model using current settings and provided vocabulary size.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vocab_size : int, optional\n",
      " |          Number of unique tokens in the vocabulary\n",
      " |      report : dict of (str, int), optional\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict of (str, int)\n",
      " |          A dictionary from string representations of the model's memory consuming members to their size in bytes.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool\n",
      " |          If True, forget the original vectors and only keep the normalized ones to save RAM.\n",
      " |  \n",
      " |  load_binary_data(self, encoding='utf8')\n",
      " |      Load data from a binary file created by Facebook's native FastText.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      encoding : str, optional\n",
      " |          Specifies the encoding.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save the Fasttext model. This saved model can be loaded again using\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`, which supports incremental training\n",
      " |      and getting vectors for out-of-vocabulary words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Store the model to this file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.load`\n",
      " |          Load :class:`~gensim.models.fasttext.FastText` model.\n",
      " |  \n",
      " |  train(self, sentences=None, corpus_file=None, total_examples=None, total_words=None, epochs=None, start_alpha=None, end_alpha=None, word_count=0, queue_factor=2, report_delay=1.0, callbacks=(), **kwargs)\n",
      " |      Update the model's neural weights from a sequence of sentences (can be a once-only generator stream).\n",
      " |      For FastText, each sentence must be a list of unicode strings.\n",
      " |      \n",
      " |      To support linear learning-rate decay from (initial) `alpha` to `min_alpha`, and accurate\n",
      " |      progress-percentage logging, either `total_examples` (count of sentences) or `total_words` (count of\n",
      " |      raw words in sentences) **MUST** be provided. If `sentences` is the same corpus\n",
      " |      that was provided to :meth:`~gensim.models.fasttext.FastText.build_vocab` earlier,\n",
      " |      you can simply use `total_examples=self.corpus_count`.\n",
      " |      \n",
      " |      To avoid common mistakes around the model's ability to do multiple training passes itself, an\n",
      " |      explicit `epochs` argument **MUST** be provided. In the common and recommended case\n",
      " |      where :meth:`~gensim.models.fasttext.FastText.train` is only called once, you can set `epochs=self.iter`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      sentences : iterable of list of str, optional\n",
      " |          The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      " |          consider an iterable that streams the sentences directly from disk/network.\n",
      " |          See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      " |          or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      " |      corpus_file : str, optional\n",
      " |          Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      " |          If you use this argument instead of `sentences`, you must provide `total_words` argument as well. Only one\n",
      " |          of `sentences` or `corpus_file` arguments need to be passed (not both of them).\n",
      " |      total_examples : int\n",
      " |          Count of sentences.\n",
      " |      total_words : int\n",
      " |          Count of raw words in sentences.\n",
      " |      epochs : int\n",
      " |          Number of iterations (epochs) over the corpus.\n",
      " |      start_alpha : float, optional\n",
      " |          Initial learning rate. If supplied, replaces the starting `alpha` from the constructor,\n",
      " |          for this one call to :meth:`~gensim.models.fasttext.FastText.train`.\n",
      " |          Use only if making multiple calls to :meth:`~gensim.models.fasttext.FastText.train`, when you want to manage\n",
      " |          the alpha learning-rate yourself (not recommended).\n",
      " |      end_alpha : float, optional\n",
      " |          Final learning rate. Drops linearly from `start_alpha`.\n",
      " |          If supplied, this replaces the final `min_alpha` from the constructor, for this one call to\n",
      " |          :meth:`~gensim.models.fasttext.FastText.train`.\n",
      " |          Use only if making multiple calls to :meth:`~gensim.models.fasttext.FastText.train`, when you want to manage\n",
      " |          the alpha learning-rate yourself (not recommended).\n",
      " |      word_count : int\n",
      " |          Count of words already trained. Set this to 0 for the usual\n",
      " |          case of training on all words in sentences.\n",
      " |      queue_factor : int\n",
      " |          Multiplier for size of queue (number of workers * queue_factor).\n",
      " |      report_delay : float\n",
      " |          Seconds to wait before reporting progress.\n",
      " |      callbacks : :obj: `list` of :obj: `~gensim.models.callbacks.CallbackAny2Vec`\n",
      " |          List of callbacks that need to be executed/run at specific stages during training.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.models import FastText\n",
      " |          >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      " |          >>>\n",
      " |          >>> model = FastText(min_count=1)\n",
      " |          >>> model.build_vocab(sentences)\n",
      " |          >>> model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(*args, **kwargs) from builtins.type\n",
      " |      Load a previously saved `FastText` model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the saved file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.fasttext.FastText`\n",
      " |          Loaded model.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.fasttext.FastText.save`\n",
      " |          Save :class:`~gensim.models.fasttext.FastText` model.\n",
      " |  \n",
      " |  load_fasttext_format(model_file, encoding='utf8', full_model=True) from builtins.type\n",
      " |      Load the input-hidden weight matrix from Facebook's native fasttext `.bin` and `.vec` output files.\n",
      " |      \n",
      " |      By default, this function loads the full model.  A full model allows\n",
      " |      continuing training with more data, but also consumes more RAM and\n",
      " |      takes longer to load.  If you do not need to continue training and only\n",
      " |      wish the work with the already-trained embeddings, use `full_model=False`\n",
      " |      for faster loading and to save RAM.\n",
      " |      \n",
      " |      Notes\n",
      " |      ------\n",
      " |      Facebook provides both `.vec` and `.bin` files with their modules.\n",
      " |      The former contains human-readable vectors.\n",
      " |      The latter contains machine-readable vectors along with other model parameters.\n",
      " |      This function effectively ignores `.vec` output file, since that file is redundant.\n",
      " |      It only needs the `.bin` file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model_file : str\n",
      " |          Path to the FastText output files.\n",
      " |          FastText outputs two model files - `/path/to/model.vec` and `/path/to/model.bin`\n",
      " |          Expected value for this example: `/path/to/model` or `/path/to/model.bin`,\n",
      " |          as Gensim requires only `.bin` file to the load entire fastText model.\n",
      " |      encoding : str, optional\n",
      " |          Specifies the file encoding.\n",
      " |      full_model : boolean, optional\n",
      " |          If False, skips loading the hidden output matrix. This saves a fair bit\n",
      " |          of CPU time and RAM, but **prevents training continuation**.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Load, infer, continue training:\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> from gensim.test.utils import datapath\n",
      " |          >>>\n",
      " |          >>> cap_path = datapath(\"crime-and-punishment.bin\")\n",
      " |          >>> fb_full = FastText.load_fasttext_format(cap_path, full_model=True)\n",
      " |          >>>\n",
      " |          >>> 'landlord' in fb_full.wv.vocab  # Word is out of vocabulary\n",
      " |          False\n",
      " |          >>> oov_term = fb_full.wv['landlord']\n",
      " |          >>>\n",
      " |          >>> 'landlady' in fb_full.wv.vocab  # Word is in the vocabulary\n",
      " |          True\n",
      " |          >>> iv_term = fb_full.wv['landlady']\n",
      " |          >>>\n",
      " |          >>> new_sent = [['lord', 'of', 'the', 'rings'], ['lord', 'of', 'the', 'flies']]\n",
      " |          >>> fb_full.build_vocab(new_sent, update=True)\n",
      " |          >>> fb_full.train(sentences=new_sent, total_examples=len(new_sent), epochs=5)\n",
      " |      \n",
      " |      Load quickly, infer (forego training continuation):\n",
      " |      \n",
      " |      .. sourcecode:: pycon\n",
      " |      \n",
      " |          >>> fb_partial = FastText.load_fasttext_format(cap_path, full_model=False)\n",
      " |          >>>\n",
      " |          >>> 'landlord' in fb_partial.wv.vocab  # Word is out of vocabulary\n",
      " |          False\n",
      " |          >>> oov_term = fb_partial.wv['landlord']\n",
      " |          >>>\n",
      " |          >>> 'landlady' in fb_partial.wv.vocab  # Word is in the vocabulary\n",
      " |          True\n",
      " |          >>> iv_term = fb_partial.wv['landlady']\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      gensim.models.fasttext.FastText\n",
      " |          The loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  bucket\n",
      " |  \n",
      " |  max_n\n",
      " |  \n",
      " |  min_n\n",
      " |  \n",
      " |  num_ngram_vectors\n",
      " |  \n",
      " |  syn0_ngrams_lockf\n",
      " |  \n",
      " |  syn0_vocab_lockf\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Get a human readable representation of the object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          A human readable string containing the class name, as well as the size of dictionary, number of\n",
      " |          features and starting learning rate used by the object.\n",
      " |  \n",
      " |  build_vocab_from_freq(self, word_freq, keep_raw_vocab=False, corpus_count=None, trim_rule=None, update=False)\n",
      " |      Build vocabulary from a dictionary of word frequencies.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_freq : dict of (str, int)\n",
      " |          A mapping from a word in the vocabulary to its frequency count.\n",
      " |      keep_raw_vocab : bool, optional\n",
      " |          If False, delete the raw vocabulary after the scaling is done to free up RAM.\n",
      " |      corpus_count : int, optional\n",
      " |          Even if no corpus is provided, this argument can set corpus_count explicitly.\n",
      " |      trim_rule : function, optional\n",
      " |          Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      " |          be trimmed away, or handled using the default (discard if word count < min_count).\n",
      " |          Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      " |          or a callable that accepts parameters (word, count, min_count) and returns either\n",
      " |          :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      " |          The rule, if given, is only used to prune vocabulary during current method call and is not stored as part\n",
      " |          of the model.\n",
      " |      \n",
      " |          The input parameters are of the following types:\n",
      " |              * `word` (str) - the word we are examining\n",
      " |              * `count` (int) - the word's frequency count in the corpus\n",
      " |              * `min_count` (int) - the minimum count threshold.\n",
      " |      \n",
      " |      update : bool, optional\n",
      " |          If true, the new provided words in `word_freq` dict will be added to model's vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Deprecated, use self.wv.doesnt_match() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.doesnt_match`.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Deprecated, use self.wv.evaluate_word_pairs() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.evaluate_word_pairs`.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Deprecated, use self.wv.most_similar() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Deprecated, use self.wv.most_similar_cosmul() instead.\n",
      " |      \n",
      " |      Refer to the documentation for\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar_cosmul`.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Deprecated, use self.wv.n_similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.n_similarity`.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_vector() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_vector`.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Deprecated, use self.wv.similar_by_word() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similar_by_word`.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Deprecated, use self.wv.similarity() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Deprecated, use self.wv.wmdistance() instead.\n",
      " |      \n",
      " |      Refer to the documentation for :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.wmdistance`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.models.base_any2vec.BaseWordEmbeddingsModel:\n",
      " |  \n",
      " |  cum_table\n",
      " |  \n",
      " |  hashfxn\n",
      " |  \n",
      " |  iter\n",
      " |  \n",
      " |  layer1_size\n",
      " |  \n",
      " |  min_count\n",
      " |  \n",
      " |  sample\n",
      " |  \n",
      " |  syn0_lockf\n",
      " |  \n",
      " |  syn1\n",
      " |  \n",
      " |  syn1neg\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we determine the number of CPUs that are available on our machine  \n",
    "(The more cores are available, the faster we can train our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(sg = 0, cbow_mean = 1, size = 100, alpha = 0.025, min_alpha = 0.0001, min_n = 3, max_n = 5,\n",
    "                    window = 5, min_count = 5, sample = 0.001, negative = 5, workers = cpus - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model with our twitter data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.build_vocab(sentences = tweets, update = False, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model:  \n",
    "(Hint: If you want to compre the runtime of the model for different number of cores or epochs, just put \"%timeit\" in front of the command  \n",
    " in the next chunk. You will then get an evaluation of how long the process takes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.train(sentences = tweets, total_examples = ft_model.corpus_count, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"example\" in ft_model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.12539083, -1.2392306 ,  0.70108515,  0.46797296,  0.8685001 ,\n",
       "       -0.95858866,  0.33056358,  1.0688312 ,  0.1591232 , -0.87566835,\n",
       "       -0.8907124 , -0.9613014 ,  0.01457906,  0.18087572,  0.13759342,\n",
       "        0.04009672,  0.59700274, -0.21330199, -0.82340777, -0.2022132 ,\n",
       "       -0.9784647 ,  0.3887765 , -0.04538687,  0.21417032, -0.52254695,\n",
       "        0.1549123 , -0.5647041 ,  0.801213  , -0.3639199 , -0.97105604,\n",
       "        0.0258203 ,  1.505556  , -0.07820963, -0.543697  , -0.04845   ,\n",
       "       -0.42218333, -0.9409691 ,  0.16737086, -0.8970547 , -0.6361535 ,\n",
       "       -0.86656135,  0.22954378,  0.02725276, -1.3792439 ,  1.2192922 ,\n",
       "       -0.28787604,  0.9299774 ,  0.14909214, -0.7312445 ,  1.2596277 ,\n",
       "       -1.0304984 , -0.26936567, -1.1585648 , -0.65198857,  0.0481079 ,\n",
       "       -0.08059145, -0.38537207, -0.6508886 ,  0.27607346, -0.14374235,\n",
       "       -1.2735661 , -1.2851834 ,  0.20870413, -0.6537805 ,  0.24656989,\n",
       "       -0.38214043, -1.047298  , -0.1520072 , -1.1581744 ,  0.24300365,\n",
       "        0.08524518, -0.02148962,  0.03417402,  0.6277424 , -0.09284835,\n",
       "        0.43788043,  0.11924458,  0.18731365, -0.68451905, -2.6749246 ,\n",
       "       -0.0744924 ,  0.30239815, -0.89782625, -1.2978092 ,  0.25818783,\n",
       "        0.05536036,  0.33884057, -0.27117577,  0.4878832 , -0.15807177,\n",
       "       -0.6565992 ,  0.446672  ,  0.29444322, -0.3145528 ,  0.22450553,\n",
       "       -0.11917987,  0.5849031 ,  0.51715004,  0.57710475,  0.2341827 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv[\"example\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simple', 0.6218103170394897),\n",
       " ('completely', 0.41479939222335815),\n",
       " ('complete', 0.37091994285583496),\n",
       " ('exactly', 0.37088626623153687),\n",
       " ('exact', 0.3245594799518585),\n",
       " ('completed', 0.3221316933631897),\n",
       " ('apple', 0.3216664791107178),\n",
       " ('nice', 0.3191584348678589),\n",
       " ('texas', 0.31394606828689575),\n",
       " ('simply', 0.2968178391456604)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv.most_similar(positive = [\"example\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now: Explore your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('many', 0.7087990045547485), ('countries', 0.4370884299278259), ('interest', 0.4294402599334717), ('currency', 0.4294150769710541), ('any', 0.42037394642829895), ('fentanyl', 0.41558194160461426), ('interesting', 0.40786013007164), ('company', 0.4027581810951233), ('european', 0.39604395627975464), ('6', 0.3857548236846924)]\n",
      "[('crooked', 0.6381134986877441), ('hilton', 0.5869311690330505), ('hillary', 0.5161709189414978), ('dnc', 0.4843655526638031), ('33000', 0.46502190828323364), ('acid', 0.4564186930656433), ('classified', 0.4558030962944031), ('campaign', 0.4372970461845398), ('deleted', 0.4247213900089264), ('climate', 0.4107172191143036)]\n",
      "[('democracy', 0.8606078624725342), ('democrat', 0.8518213033676147), ('democratic', 0.8404862284660339), ('dems', 0.7639465928077698), ('they', 0.4987036883831024), ('demean', 0.4773533046245575), ('demand', 0.4628133773803711), ('debt', 0.44863998889923096), ('committees', 0.3985235095024109), ('means', 0.3982166647911072)]\n",
      "[('rico', 0.44321587681770325), ('immigration', 0.4417235255241394), ('solution', 0.4002934992313385), ('legislation', 0.39332088828086853), ('resolution', 0.38773784041404724), ('southern', 0.37935584783554077), ('starting', 0.37600162625312805), ('star', 0.37326863408088684), ('china', 0.37190860509872437), ('trade', 0.3709579110145569)]\n",
      "[('chinese', 0.7521058917045593), ('chief', 0.47732990980148315), ('agriculture', 0.4500102996826172), ('company', 0.44971993565559387), ('watching', 0.44301390647888184), ('eu', 0.4387896656990051), ('money', 0.4383842349052429), ('existing', 0.4240643084049225), ('agricultural', 0.4153945744037628), ('tariffs', 0.41297581791877747)]\n",
      "[('transparency', 0.4459735155105591), ('usmca', 0.43068355321884155), ('renegotiate', 0.39372768998146057), ('train', 0.3870087265968323), ('maduro', 0.3801177144050598), ('advanced', 0.3739730417728424), ('manipulation', 0.37297070026397705), ('eu', 0.3657553493976593), ('traditional', 0.3644499182701111), ('made', 0.3625670075416565)]\n"
     ]
    }
   ],
   "source": [
    "print(ft_model.wv.most_similar(positive = [\"germany\"]))\n",
    "print(ft_model.wv.most_similar(positive = [\"clinton\"]))\n",
    "print(ft_model.wv.most_similar(positive = [\"democrats\"]))\n",
    "print(ft_model.wv.most_similar(positive = [\"mexico\"]))\n",
    "print(ft_model.wv.most_similar(positive = [\"china\"]))\n",
    "print(ft_model.wv.most_similar(positive = [\"mexico\", \"trade\"], negative = [\"wall\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the possibilities the model by e.g. switching from skip-gram to cbow, using averaging instead of concatenation, chosing a larger embedding size, more negative examples, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using ``gensim.models.phrases`` in order to form bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "phrases = Phrases(tweets, min_count=100, threshold=10)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((b'\\xe2\\x80\\x99', b't'), 40.14282388966591),\n",
       " ((b'\\xe2\\x80\\x99', b's'), 39.051824673367356),\n",
       " ((b'witch', b'hunt'), 24.661771250556296),\n",
       " ((b'will', b'be'), 15.202834497541446),\n",
       " ((b'united', b'states'), 98.71593416819549),\n",
       " ((b'thank', b'you'), 49.87788116523749),\n",
       " ((b'our', b'country'), 27.60977332033378),\n",
       " ((b'fake', b'news'), 74.46145685997172),\n",
       " ((b'don', b'\\xe2\\x80\\x99'), 16.771136693276688)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(bigram.phrasegrams.items()), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(bigram[tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9408352 , -2.9448547 , -0.0644585 ,  2.5617635 ,  0.11626738,\n",
       "        0.9916419 , -0.982047  , -0.36746535,  2.282364  ,  4.807505  ,\n",
       "       -0.57372135, -0.1776367 ,  3.6928544 , -0.07633026, -0.3486168 ,\n",
       "       -1.1801438 , -2.0244286 ,  3.9215038 ,  5.7360845 ,  1.1472751 ,\n",
       "       -0.6600361 ,  3.4406743 ,  0.42411965,  2.2777426 ,  4.44871   ,\n",
       "       -0.51950186, -3.310043  , -0.11698956,  1.4302472 , -1.8335285 ,\n",
       "       -1.2929436 , -3.6467178 ,  0.5088786 ,  0.97771716, -2.7727382 ,\n",
       "       -1.151348  , -2.8728178 ,  0.28081998,  0.15078373,  0.28773436,\n",
       "        2.4083438 ,  1.5199484 ,  0.94393754, -3.9986844 , -0.9181879 ,\n",
       "        2.4555998 , -0.99208266,  0.16184539, -2.7211847 , -0.95676094,\n",
       "       -1.9936352 ,  0.18663087,  0.7953555 , -1.7466047 ,  1.345637  ,\n",
       "        0.13392718, -0.39346752, -1.2971301 ,  1.736669  ,  1.0345834 ,\n",
       "        3.0065439 , -0.919131  , -0.42990413,  1.9712603 , -1.983691  ,\n",
       "       -0.59394854, -1.0466216 ,  2.0349941 ,  2.022733  , -0.22590888,\n",
       "        0.11601094,  4.5244036 , -1.2167819 , -1.489552  , -0.08983883,\n",
       "        0.09235708,  1.7298062 , -1.4144298 ,  4.906148  ,  1.6651174 ,\n",
       "        2.6185067 ,  1.9192597 , -1.6644619 , -0.79504645,  1.172478  ,\n",
       "        1.7839055 , -0.94101334,  5.75815   ,  3.9923134 , -2.2030773 ,\n",
       "        1.2307607 ,  0.41266924,  0.5325593 ,  0.6659662 ,  1.2994248 ,\n",
       "        0.14457057,  1.6736019 , -2.932189  , -1.2620611 , -0.7192462 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"clinton\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
