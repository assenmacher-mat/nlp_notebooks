{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Exercise 3 - FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports\n",
    "\n",
    "In order to handle the data properly we have to import the data and the modules we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.fasttext import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you need to download the data set \"tweets.csv\" from the GitHub repository https://github.com/assenmacher-mat/nlp_notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook on colab ( https://colab.research.google.com/ ), you also need to run the next chunk in order to upload the data to colab.  \n",
    "Choose it in the upload window and in it will be available on colab from now on.__  \n",
    "(If you are running this notebook locally on your machine, you can skip the execution of this chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data set and perform pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are running this notebook locally on your machine, you might need to adjust the path (depending on where you've saved the data).__  \n",
    "(If you are running this notebook on colab, you can can leave the path unchanged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = pd.read_csv(\"trump.csv\")\n",
    "tweets_raw = [tweet for tweet in list(tweet_data.text)]\n",
    "tweets = [doc.lower() for doc in tweets_raw]\n",
    "tweets = [re.sub(r\"https://.*|“|”|@\", \"\", doc) for doc in tweets]\n",
    "tweets = [re.sub(r\"[\\)\\(\\.\\,;:!?\\+\\-\\_\\#\\'\\*\\§\\$\\%\\&]\", \"\", doc) for doc in tweets]\n",
    "tweets = [nltk.word_tokenize(doc) for doc in tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After all, we can finally start with the modeling part!  \n",
    "(If you want to have a look at the help page, just execute the following chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we determine the number of CPUs that are available on our machine  \n",
    "(The more cores are available, the faster we can train our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = FastText(sg = 0, cbow_mean = 1, size = 100, alpha = 0.025, min_alpha = 0.0001, min_n = 3, max_n = 5,\n",
    "                    window = 5, min_count = 5, sample = 0.001, negative = 5, workers = cpus - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model with our twitter data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model.build_vocab(sentences = tweets, update = False, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model:  \n",
    "(Hint: If you want to compre the runtime of the model for different number of cores or epochs, just put \"%timeit\" in front of the command  \n",
    " in the next chunk. You will then get an evaluation of how long the process takes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.6 s ± 310 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit ft_model.train(sentences = tweets, total_examples = ft_model.corpus_count, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word2vec model with the same model specifications (skip-gram, vector size, window, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7088596, 10534100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = Word2Vec(sg = 0, cbow_mean = 1, size = 100, alpha = 0.025, min_alpha = 0.0001, \n",
    "                     window = 5, min_count = 5, sample = 0.001, negative = 5, workers = cpus - 1)\n",
    "w2v_model.build_vocab(sentences = tweets, update = False, progress_per = 10000)\n",
    "w2v_model.train(sentences = tweets, total_examples = w2v_model.corpus_count, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check, whether the word \"example\" does occur in your model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"example\" in ft_model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to query your word2vec model for a vector representation of this word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'example' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-3c4f0e1ce939>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"example\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\ri85vex\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ri85vex\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ri85vex\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'example' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "w2v_model.wv[\"example\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try to query your fastText model for a vector representation of this word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0486553e+00,  1.6520401e+00,  2.1554410e+00,  1.1430428e+00,\n",
       "        1.6350437e+00, -2.8443673e+00,  3.1840882e+00,  1.6516072e+00,\n",
       "        1.7214676e+00, -2.5208766e+00,  1.3167694e+00, -2.6342528e+00,\n",
       "       -2.3123934e+00, -2.2165632e-01, -6.5016836e-01, -9.7515565e-01,\n",
       "        2.1689889e+00,  1.9777322e+00, -7.1687174e-01,  7.2892499e-01,\n",
       "        1.5738661e-03, -6.3561291e-01,  2.0284667e+00,  2.1132912e-01,\n",
       "       -1.2740127e+00, -3.0367064e+00, -1.0260161e+00,  2.2499128e-01,\n",
       "        6.3597220e-01, -7.5686151e-01, -1.0237803e+00,  3.9474463e+00,\n",
       "       -1.8808597e+00, -7.2630548e-01, -1.4895096e+00, -2.0428224e+00,\n",
       "       -6.9836181e-01,  3.1248198e+00, -1.0949135e-01,  4.1144320e-01,\n",
       "       -9.1552299e-01, -9.9182791e-01,  4.2851403e-01, -4.1289307e-02,\n",
       "        1.3214664e+00, -8.6032259e-01, -2.4536220e-02,  1.2301015e+00,\n",
       "        6.5257055e-01, -1.3914090e+00, -1.9630507e+00,  1.6500744e+00,\n",
       "       -2.0449514e+00,  6.4786309e-01, -1.2088287e+00, -1.9017792e+00,\n",
       "        2.3425274e+00,  8.7034988e-01, -3.6690426e-01,  1.3035084e+00,\n",
       "       -1.3936261e+00, -2.4731543e+00, -4.4776729e-01, -8.7751456e-02,\n",
       "       -8.2660753e-01,  2.0816908e+00,  1.0571517e+00,  7.4104327e-01,\n",
       "       -4.9116307e-01,  9.0319639e-01,  1.0031511e+00, -1.8511162e+00,\n",
       "        1.7361249e+00,  2.6527717e+00, -2.1320040e+00, -2.4989821e-01,\n",
       "        9.3914467e-01,  1.6629618e-01,  7.7497232e-01, -2.4643822e+00,\n",
       "        1.7822753e+00,  8.8138443e-01, -1.0478176e+00, -1.0854201e+00,\n",
       "       -4.8416898e-02, -1.6097122e+00, -4.7739041e-01,  1.3770468e+00,\n",
       "       -9.5279223e-01, -1.4759984e+00, -1.6484162e+00,  1.9419216e+00,\n",
       "       -6.8331057e-01, -1.2410126e+00, -1.1838280e+00,  2.7946210e+00,\n",
       "        9.9823767e-01, -3.0391034e-01, -3.0202428e-01, -7.6705110e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv[\"example\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the most word with the most similar vector representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('simple', 0.5714014768600464),\n",
       " ('exact', 0.4678761959075928),\n",
       " ('exactly', 0.46437159180641174),\n",
       " ('texas', 0.31973931193351746),\n",
       " ('india', 0.30366888642311096),\n",
       " ('ohr', 0.2940193712711334),\n",
       " ('having', 0.29206737875938416),\n",
       " ('aspect', 0.2863953709602356),\n",
       " ('justices', 0.2860548198223114),\n",
       " ('inside', 0.28579258918762207)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv.most_similar(positive = [\"example\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check, whether the word \"democrats\" does occur in your model's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"democrats\" in ft_model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to query your word2vec model for a vector representation of this word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.56760174,  1.0978279 , -0.44636375, -1.370232  ,  0.6363281 ,\n",
       "       -0.6500159 ,  1.0603571 ,  3.1058614 ,  2.14952   , -0.6802631 ,\n",
       "       -1.0683842 , -0.8041357 , -2.0587215 , -1.1914525 , -1.0351582 ,\n",
       "       -4.352592  ,  0.11410102,  1.0767232 ,  1.997807  ,  1.2732118 ,\n",
       "        0.50725424, -0.74185264,  1.2317436 , -0.60479367, -0.09600411,\n",
       "       -0.62712747,  0.8109986 , -1.9294683 , -2.543967  , -1.3310887 ,\n",
       "        1.2558452 , -0.47315744, -1.4003739 ,  2.3724196 ,  1.880108  ,\n",
       "        0.14799374, -2.268063  ,  2.5570033 , -0.8379476 ,  0.38235664,\n",
       "       -0.41780508,  0.63027084,  0.606201  ,  0.92862767,  0.18029945,\n",
       "        1.3708873 ,  2.6459687 ,  1.7692064 ,  2.7067606 , -0.01189727,\n",
       "       -0.15033926,  0.9917862 , -4.098683  ,  3.9586165 ,  0.5021969 ,\n",
       "       -3.3960874 ,  2.5581117 , -4.6729093 ,  0.5639138 ,  0.6061467 ,\n",
       "       -1.182844  ,  2.1353543 , -1.5632311 ,  2.1959524 , -1.0714059 ,\n",
       "        0.28672642, -0.28899705,  1.641964  ,  0.34226125,  0.20523936,\n",
       "       -0.9897093 ,  3.006126  , -0.15926404, -0.9512342 , -0.04588167,\n",
       "       -1.3794194 , -1.1742058 , -1.47286   ,  1.0844666 , -1.86119   ,\n",
       "        1.2410102 ,  0.7434908 , -0.02985559,  0.07187038,  0.29373908,\n",
       "        0.66897607, -0.37701842, -0.59186995,  1.3642557 , -1.3066194 ,\n",
       "       -0.84058464,  2.3201768 ,  1.6988517 ,  1.9623863 ,  0.20168634,\n",
       "       -3.448527  ,  0.26725736, -2.2719035 ,  1.6044612 , -1.7175055 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv[\"democrats\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now try to query your fastText model for a vector representation of this word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8398814 , -1.8945438 ,  1.1597911 , -0.81199354,  0.6732897 ,\n",
       "       -1.401981  , -0.438043  , -0.23753382, -0.65049314,  0.20491856,\n",
       "       -0.11132   ,  1.7340851 , -0.33728626,  0.4092576 , -0.0639997 ,\n",
       "        1.2539299 , -2.389616  ,  0.8441182 ,  0.6235202 ,  0.57921076,\n",
       "        1.2038391 ,  2.2296095 ,  0.3568454 , -0.93572885,  1.2726535 ,\n",
       "       -0.39360413,  0.88321304,  0.56413466, -0.18699293, -1.6253219 ,\n",
       "       -0.8681686 , -1.036386  , -1.2211905 , -0.15744247, -1.3042277 ,\n",
       "        0.50965905,  0.36817193,  0.20597057,  1.7013777 ,  0.22775795,\n",
       "       -0.8392609 , -0.25008392, -0.07426292, -1.6004959 , -2.1281245 ,\n",
       "        0.31867576,  0.46663764, -1.0829184 , -1.9937471 ,  0.04136777,\n",
       "       -0.6111922 ,  0.50176954, -0.3769376 , -1.9965129 ,  1.1369956 ,\n",
       "        0.09730179, -0.6319924 ,  0.82005835, -0.5232716 , -0.7630313 ,\n",
       "       -1.0930935 ,  1.9585453 ,  2.076092  ,  0.48397627,  0.14669396,\n",
       "       -2.3992023 ,  0.90727973,  0.4698415 , -0.26430663,  0.48808843,\n",
       "       -1.8856267 ,  0.06680615,  1.0804152 ,  0.49369013,  0.79351914,\n",
       "       -2.2360358 , -1.583445  ,  0.17051893, -0.31617245, -0.36712956,\n",
       "        3.1913564 ,  0.9944812 ,  0.575909  ,  2.117816  , -0.02791359,\n",
       "       -0.30248013, -1.5961529 ,  1.000873  ,  0.6336537 , -0.86403686,\n",
       "        0.20730847, -1.4599658 , -0.5379564 ,  0.48765418, -0.04132949,\n",
       "       -0.5446347 ,  0.07729519, -0.19286606, -0.19444226,  0.7821258 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv[\"democrats\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the most word with the most similar vector representations (for each of the two models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dems', 0.7657046318054199),\n",
       " ('they', 0.479423463344574),\n",
       " ('democrat', 0.44336768984794617),\n",
       " ('we', 0.4397535026073456),\n",
       " ('losers', 0.3709675669670105),\n",
       " ('people', 0.33017468452453613),\n",
       " ('fix', 0.3238281011581421),\n",
       " ('others', 0.32353824377059937),\n",
       " ('you', 0.31534522771835327),\n",
       " ('actions', 0.31037071347236633)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive = [\"democrats\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dems', 0.6397151350975037),\n",
       " ('democrat', 0.5384020209312439),\n",
       " ('they', 0.496374249458313),\n",
       " ('we', 0.4879452586174011),\n",
       " ('democracy', 0.44930195808410645),\n",
       " ('democratic', 0.34020522236824036),\n",
       " ('you', 0.32630443572998047),\n",
       " ('people', 0.31717851758003235),\n",
       " ('drugs', 0.2760796546936035),\n",
       " ('committees', 0.268619567155838)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.wv.most_similar(positive = [\"democrats\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you recognize any systematic differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the possibilities the model by e.g. switching from skip-gram to cbow, trying different n-gram ranges, chosing a larger embedding size, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
